# Web-Crawler
Crawled www.latimes.com and extracted relevant data. Also analyzed it using python.

Here in this project, a crawler is developed to crawl the website www.latimes.com
The crawler4j api has been used.
This crawler starts with "www.latimes.com" as the seed and records and analyzes all the links on that page. Only Html, doc, pdf, jpeg, jpg, png, tiff files have been crawled.
The stats.py file is a statistics file which gives insights about the data is scripted in python and pandas has been used.
All the crawlReport_latimes.txt is the output of the stats.py file.
The output from the crawler can be viewed in the fetch.csv and visit.csv files.
The crawler was run in the eclipse ide which made it easy to load the necessary jar dependencies.

This crawler can also be used for crawling any other site and modifying the parameters inorder to process other type of files too.
